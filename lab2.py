# -*- coding: utf-8 -*-
"""lab2-0860908.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jJK3PMgZTTm78vMYwC994Fa4qQ_IChbH
"""

# import nltk

# from nltk.corpus import stopwords
# nltk.download('stopwords')
# # stopwords.words('english')

# from nltk.tokenize import word_tokenize
# nltk.download('punkt')

# print(word_tokenize("It's wonderful to be in Taiwan."))
# print(word_tokenize("It's not so wonderful to be in U.S. right now."))
# print(word_tokenize("Taiwan gives 400,000 masks to U.S. under cooperation arrangment."))
# print(word_tokenize("Taiwan gives 400,000 masks to U.S. under cooperation arrangment. #TaiwanCanHelp :)"))

# tokens = word_tokenize("Taiwan gives 400,000 masks to U.S. under cooperation arrangment. #TaiwanCanHelp :)")
# tokens = [token for token in tokens if token not in stopwords.words('english')]
# tokens

# from nltk.tokenize import TweetTokenizer
# tweet_tokenizer = TweetTokenizer()
# print(tweet_tokenizer.tokenize("Taiwan gives 400,000 masks to U.S. under cooperation arrangment. #TaiwanCanHelp :)"))

# from nltk.tokenize import sent_tokenize
# sent_tokenize("Hello, and welcome to the U.S.! How are you? I feel so happy... And you?")

# from nltk import ngrams
# list(ngrams(tokens, 3))

# from nltk import pos_tag
# nltk.download('averaged_perceptron_tagger')

# pos_tag(tokens)

# import spacy
# nlp = spacy.load("en_core_web_sm")

# doc = nlp("Taiwan gives NT $400,000 masks to U.S. under cooperation arrangment. #TaiwanCanHelp :)")

# for token in doc:
#   print(token.text, f'({token.pos_}) \n', f'lemma: {token.lemma_}, ', 
#         f'alpha: {token.is_alpha}, ', f'stop: {token.is_stop}')

"""# Assignment 2

## - **5 most frequent 2-grams**
"""

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.tokenize import sent_tokenize
from nltk import pos_tag
from nltk import ngrams
import pandas as pd


nltk.download('stopwords')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

def get_corpus(csv_path):
  df = pd.read_csv(csv_path, encoding='utf-8')
  print("Dataset size: ", len(df))
  print("Dataset columns", df.columns)

  corpus = df.to_dict('records')

  return corpus

def tokenize(corpus):
  content_token = list()

  for doc in corpus:
    sentences = sent_tokenize(doc['content'])
    content_token.append([word_tokenize(sentence) for sentence in sentences])

  return content_token

def bag_Of_Ngrams(tokens):
  bag = dict()

  for content in tokens:
    for sentence_token in content:
      _2_gram = list(ngrams(sentence_token, 2))
      _2_gram_NNP = [
        gram for gram in _2_gram
          if (pos_tag(gram)[0][1] == 'NNP' or pos_tag(gram)[0][1] == 'NNPS') and
            (pos_tag(gram)[1][1] == 'NNP' or pos_tag(gram)[1][1] == 'NNPS')
      ]

      # print(_2_gram)

      for gram in _2_gram_NNP:
        if gram in bag.keys():
          bag[gram] += 1
        else:
          bag[gram] = 1

  return bag

corpus = get_corpus("https://raw.githubusercontent.com/bshmueli/108-nlp/master/reuters.csv")

tokens = tokenize(corpus)
bag = bag_Of_Ngrams(tokens)

bag = {k: v for k, v in sorted(bag.items(), key=lambda item: item[1], reverse=True)}
bag_items = list(bag.items())
for i in range(5):
  print('> ', bag_items[i][0], ': ', bag_items[i][1])

"""## - **5 most similar articles to seed_id = 908**"""

import spacy
nlp = spacy.load("en_core_web_sm")

from collections import Counter
def get_vocab(corpus):
  nlp = spacy.load("en_core_web_sm")
  vocabulary = Counter()

  for document in corpus:
    # print(document['title'].replace(u'\xa0', u' '))
    tokens = nlp(str(document['content']).replace(u'\xa0', u' '))
    tokens = [token.lemma_ + '_' + token.pos_ for token in tokens if not token.is_stop]
    # print(tokens)
    vocabulary.update(tokens)

  return vocabulary

import math
def tfidf_doc2vec(doc, vocab, corpus, corpus_count):
  doc_vocab = dict(get_vocab([doc]))

  doc_num_of_words = sum(doc_vocab.values())

  vocab = vocab.most_common(512)

  vec = list()
  for word in dict(vocab).keys():
    if word in doc_vocab.keys():
      if_value = doc_vocab[word] / doc_num_of_words
      idf_value = math.log(len(corpus) / corpus_count[word])
      vec.append(if_value * idf_value)

    else:
      vec.append(0)

  return vec

def cosine_similarity(vec_a, vec_b):
  assert len(vec_a) == len(vec_b)

  if sum(vec_a) == 0 or sum(vec_b) == 0:
    return 0

  a_b = sum(i[0] * i[1] for i in zip(vec_a, vec_b))
  a_2 = sum([i * i for i in vec_a])
  b_2 = sum([i * i for i in vec_b])

  return a_b / (math.sqrt(a_2) * math.sqrt(b_2))

def doc_similarity(doc_a, doc_b, vocab, corpus, corpus_count):
  return cosine_similarity(
      tfidf_doc2vec(doc_a, vocab, corpus=corpus, corpus_count=corpus_count), 
      tfidf_doc2vec(doc_b, vocab, corpus=corpus, corpus_count=corpus_count))

def k_similar(seed_id, vocab, k=5):

  def corpus_doc_count(corpus):
    """
    find the number of documents where the term t appears
    """
    nlp = spacy.load("en_core_web_sm")
    corpus_count = Counter()

    for document in corpus:
      tokens = set(nlp(str(document['content']).replace(u'\xa0', u' ')))
      tokens = [token.lemma_ + '_' + token.pos_ for token in tokens if not token.is_stop]
      corpus_count.update(tokens)

    corpus_count = dict(corpus_count)

    return corpus_count

  seed_doc = corpus[seed_id]
  print(f"> {seed_doc['title']}")

  corpus_count = corpus_doc_count(corpus)

  similarities = [doc_similarity(
                    seed_doc, doc, vocab, corpus, corpus_count) \
                  for doc in corpus]
  top_indices = sorted(range(len(similarities)), 
                       key=lambda i: similarities[i])[-k:]

  nearest = [[corpus[id], similarities[id]] for id in top_indices]

  for story in reversed(nearest):
    print(f"* {story[0]['title']} ({story[1]})")

corpus = get_corpus("https://raw.githubusercontent.com/bshmueli/108-nlp/master/buzzfeed.csv")
vocab = get_vocab(corpus)

STUDENT_ID = 860908
lab1_index = STUDENT_ID % 1000
print(f'\nStudent ID: 0{STUDENT_ID}')
print(f'computed index: {lab1_index}\n')

k_similar(lab1_index, vocab=vocab, k=5)
