# -*- coding: utf-8 -*-
"""lab3-0860908.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Rpzx2DcYhb6Cb1KC0hC0_zZ1hEbG2Edv

# Lab 3 : Part 1
"""

import re, math
from collections import Counter, defaultdict
import nltk
import numpy as np
import pandas as pd

nltk.download('punkt')

def lowercase_and_tokenizer(corpus):
  """
  Convert the texts in corpus into lowercase, and then tokenize each text.

  Input:
    - corpus: np.array(str)
  Output
    - tokens: np.array(np.array)
  """
  tokenizer = nltk.tokenize.TweetTokenizer()

  tokens = list()
  for text in corpus:
    text = text.lower()

    sent_text = nltk.sent_tokenize(text)
    for sentence in sent_text:
      token = ['<s>'] + [word for word in tokenizer.tokenize(sentence) if word.isalnum()] + ['</s>']

      tokens.append(token)

  return tokens

def make_vocab(tokens):
  """
  Count the token.
  """
  vocab = Counter()
  
  for token in tokens:
    vocab.update(Counter(token))

  vocab = [token for token, freq in vocab.most_common() if freq >= 3]

  return vocab

def make_bigram_and_counts(tokens, vocab):
  """
  Count the bigram and replace the tokens which it appear less than 3 times with <UNK>.
  """
  forward_counts = defaultdict(lambda: defaultdict(lambda: 0))
  backward_counts = defaultdict(lambda: defaultdict(lambda: 0))
  bigrams = list()
  
  for token in tokens:

    for t in range(len(token)):
      if token[t] not in vocab:
        token[t] = '<UNK>'

    bigram = list(nltk.bigrams(token))
    bigrams.append(bigram)
    for w1, w2 in bigram:
      forward_counts[w1][w2] += 1
      backward_counts[w2][w1] += 1

  return bigrams, forward_counts, backward_counts, tokens

def average_perplexity_with_laplace_smoothing(bigrams, bigram_counts):
  """
  Compute avarage perplexity with Laplace smoothing.
  """
  perplexity = 0

  for bigram in bigrams:
    probability = np.prod([(1 + bigram_counts[w1][w2]) / (len(bigram_counts[w1].values()) + sum(bigram_counts[w1].values())) for w1, w2 in bigram])
    perplexity += math.pow(probability, -1 / (len(bigram) + 1))

  return perplexity / len(bigrams)

training_corpus = pd.read_json("http://bit.ly/nlp-tweet-train", lines=True).values[:, 0]
testing_corpus = pd.read_json("http://bit.ly/nlp-tweet-test", lines=True).values[:, 0]

training_tokens = lowercase_and_tokenizer(training_corpus)
testing_tokens = lowercase_and_tokenizer(testing_corpus)

training_vocab = make_vocab(training_tokens)

training_bigrams, training_forward_counts, training_backward_counts, training_tokens = \
  make_bigram_and_counts(
      tokens=training_tokens, 
      vocab=training_vocab
)
testing_bigrams, _, _, testing_tokens = \
  make_bigram_and_counts(
      tokens=testing_tokens, 
      vocab=training_vocab
)

training_avg_perplexity = \
  average_perplexity_with_laplace_smoothing(
      bigrams=training_bigrams, 
      bigram_counts=training_forward_counts
)
testing_avg_perplexity = \
  average_perplexity_with_laplace_smoothing(
      bigrams=testing_bigrams, 
      bigram_counts=training_forward_counts
)

print('Training average Perplexity: ', training_avg_perplexity)
print('Testing average Perplexity: ', testing_avg_perplexity)

"""# Lab 3 : Part 2"""

def average_perplexity_with_laplace_smoothing_bidirection(tokens, backward_counts, forward_counts, gamma):
  """
  Compute avarage perplexity with Laplace smoothing and bi-direction.
  """
  def forward_probability(w1, w0):
    return (1 + forward_counts[w0][w1]) / (len(forward_counts[w0].values()) + sum(forward_counts[w0].values()))

  def backward_probability(w1, w2):
    return (1 + backward_counts[w2][w1]) / (len(backward_counts[w2].values()) + sum(backward_counts[w2].values()))

  perplexity = 0

  for token in tokens:
    if len(token) < 3:
      continue

    probability = list()

    for t in range(1, len(token)-1):
      w0 = token[t-1]
      w1 = token[t]
      w2 = token[t+1]
      forward = forward_probability(w1, w0)
      backward = backward_probability(w1, w2)

      probability.append(gamma * forward + (1 - gamma) * backward)

    prob = np.prod(probability)
    if prob == 0.0:
      prob = 1e-64

    perplexity += math.pow(prob, -1 / len(token))

  return perplexity / len(tokens)

bi_training_avg_perplexity_list = list()
bi_testing_avg_perplexity_list = list()

for gamma in np.arange(0, 1.05, 0.05):
  bi_training_avg_perplexity = \
    average_perplexity_with_laplace_smoothing_bidirection(
        tokens=training_tokens, 
        forward_counts=training_forward_counts,
        backward_counts=training_backward_counts,
        gamma=gamma
  )
  bi_testing_avg_perplexity = \
    average_perplexity_with_laplace_smoothing_bidirection(
        tokens=testing_tokens, 
        forward_counts=training_forward_counts,
        backward_counts=training_backward_counts,
        gamma=gamma
  )
    
  print('Gamma: ', gamma)
  print('Training average Perplexity: ', bi_training_avg_perplexity) 
  print('Testing average Perplexity: ', bi_testing_avg_perplexity) 

  bi_training_avg_perplexity_list.append(bi_training_avg_perplexity)
  bi_testing_avg_perplexity_list.append(bi_testing_avg_perplexity)

import matplotlib.pyplot as plt

X = np.arange(0, 1.05, 0.05)
plt.figure()
plt.title('Average Perplexity')
plt.xlabel('Gamma')
plt.ylabel('average perplexity')
plt.plot(X, bi_training_avg_perplexity_list, color='b', label='training', marker='o')
plt.plot(X, bi_testing_avg_perplexity_list, color='r', label='testing', marker='o')
plt.legend() 
plt.show()

