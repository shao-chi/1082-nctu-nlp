# -*- coding: utf-8 -*-
"""lab5-Team-Queenie.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Wgax7p1zFN6z-Q76oZ9GFIv-q20OMXpV
"""

import pandas as pd
import numpy as np

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB

"""# Part-1 Data Exploration

### Number of samples that have N categories, for 1 <= N <= 6
"""

train_raw_data = pd.read_json('./drive/My Drive/data/train_gold.json', lines=True).to_dict(orient='records')

num_per_categories = np.zeros((6))
for data in train_raw_data:
    num_per_categories[len(data['categories'])-1] += 1

for n in range(len(num_per_categories)):
    print(f'N = {n+1}, {num_per_categories[n]} samples')

"""### Category distribution (how many times does each category appear)"""

from collections import Counter

categories_counter = Counter()
for data in train_raw_data:
    categories_counter.update(data['categories'])

for category, num in categories_counter.most_common():
    print(category, num)

"""### 10 most common pairs of co-occurring categories"""

categories_pair_counter = dict()
for data in train_raw_data:
    for i in range(len(data['categories'])):
        for j in range(i+1, len(data['categories'])):
            set_ = frozenset([data['categories'][i], data['categories'][j]])

            if set_ in categories_pair_counter.keys():
                categories_pair_counter[set_] += 1
            else:
                categories_pair_counter[set_] = 1

categories_pair_counter = {k: v for k, v in \
                           sorted(categories_pair_counter.items(), key=lambda item: item[1], reverse=True)}

for category, num in list(categories_pair_counter.items())[:10]:
    print(f'Pair: {list(category)[0]} {list(category)[1]}, {num}')

import seaborn as sns

categories = pd.read_json('./drive/My Drive/data/categories.json').values.reshape((-1))

matrix = dict()
for c in categories:
    matrix[c] = dict()

    for cc in categories:
        matrix[c][cc] = 0

for data in train_raw_data:
    for i in range(len(data['categories'])):
        for j in range(i+1, len(data['categories'])):
            matrix[data['categories'][i]][data['categories'][j]] += 1
            matrix[data['categories'][j]][data['categories'][i]] += 1

matrix_df = pd.DataFrame.from_records(matrix)
ax = sns.heatmap(matrix_df)

"""# Part-2 Prediction"""

dev_raw_data = pd.read_json('./drive/My Drive/data/dev_unlabeled.json', lines=True).to_dict(orient='records')

X = [data['text'] for data in train_raw_data]
Y = [np.where(categories == data['categories'][0])[0][0] for data in train_raw_data]

split = int(len(X) * 0.9)
train_X = X[:split]
train_Y = Y[:split]
train_Y_categories = train_raw_data[:split]
valid_X = X[split:]
valid_Y = Y[split:]
valid_Y_categories = train_raw_data[split:]

dev_X = [data['text'] for data in dev_raw_data]

"""## Majority Prediction (Baseline)

Use six most frequently occurring categories
"""

top_6 = list({k: v for k, v in \
          sorted(dict(categories_counter).items(), key=lambda item: item[1], reverse=True)}.keys())[:6]

train_recall = [(len(set(train_Y_categories[i]['categories']).intersection(set(top_6))) \
                 / len(train_Y_categories[i]['categories'])) \
                    for i in range(len(train_Y_categories))]

print('Training Data Recall@6: ', np.mean(train_recall))

valid_recall = [(len(set(valid_Y_categories[i]['categories']).intersection(set(top_6))) \
                 / len(valid_Y_categories[i]['categories'])) \
                    for i in range(len(valid_Y_categories))]

print('Validation Data Recall@6: ', np.mean(valid_recall))

for data in dev_raw_data:
    data['categories'] = top_6

pd.DataFrame.from_records(dev_raw_data)

!pip install ujson

import ujson

with open('./drive/My Drive/data/lab5/baseline/dev.json', 'w') as f:
    for item in dev_raw_data:
        ujson.dump(item, f)
        f.write('\n')

"""Recall@6: 0.4008541667

# Naive Bayes
"""

from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import TfidfVectorizer

tfidf = TfidfVectorizer()
train_X_tfidf = tfidf.fit_transform(train_X)
valid_X_tfidf = tfidf.transform(valid_X)
dev_X_tfidf = tfidf.transform(dev_X)

naive_bayes = MultinomialNB()
naive_bayes.fit(train_X_tfidf, train_Y)

train_proba = naive_bayes.predict_proba(train_X_tfidf)
train_proba = np.argsort(-train_proba, axis=1)
train_predict = [categories[np.where(proba < 6)[0]] for proba in train_proba]

train_recall = [(len(set(train_Y_categories[i]['categories']).intersection(set(train_predict[i]))) \
                 / len(train_Y_categories[i]['categories'])) \
                    for i in range(len(train_Y_categories))]

print('Training Data Recall@6: ', np.mean(train_recall))

valid_proba = naive_bayes.predict_proba(valid_X_tfidf)
valid_proba = np.argsort(-valid_proba, axis=1)
valid_predict = [categories[np.where(proba < 6)[0]] for proba in valid_proba]

valid_recall = [(len(set(valid_Y_categories[i]['categories']).intersection(set(valid_predict[i]))) \
                 / len(valid_Y_categories[i]['categories'])) \
                    for i in range(len(valid_Y_categories))]

print('Validation Data Recall@6: ', np.mean(valid_recall))

dev_raw_data = pd.read_json('./drive/My Drive/data/dev_unlabeled.json', lines=True).to_dict(orient='records')

dev_proba = naive_bayes.predict_proba(dev_X_tfidf)
dev_proba = np.argsort(-dev_proba, axis=1)
dev_predict = [categories[np.where(proba < 6)[0]] for proba in dev_proba]
for i in range(len(dev_raw_data)):
    dev_raw_data[i]['categories'] = list(dev_predict[i])

pd.DataFrame.from_records(dev_raw_data)

with open('./drive/My Drive/data/lab5/MultinomialNB/dev.json', 'w') as f:
    for item in dev_raw_data:
        ujson.dump(item, f)
        f.write('\n')

"""Recall@6: 0.2222875"""