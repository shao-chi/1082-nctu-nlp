# -*- coding: utf-8 -*-
"""lab1-0860908.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XkbhYzXz1szGXaNJ9h_JpRgfhBwtuNyh
"""

import re
import pandas as pd
import requests
import math

"""## Tokenizer
# Lab 1 Assignment: 
1. remove punctuation
2. Convert all tokens to lowercase
3. Remove stopwords
"""

def tokenize(sentence):
  """
  split the sentence to words.
  input: sentence (string)
  output: words (array)
  """
  words = sentence.split()

  # lowercase
  words = [w.lower() for w in words]

  # remove stopword
  stopwords = requests.get("https://raw.githubusercontent.com/bshmueli/108-nlp/master/stopwords.txt") \
                  .text.split('\n')
  for w in words:
    if w in stopwords:
      words.remove(w)

  # remove punctuation
  words = [re.sub(r'[^\w\s]', '', word) for word in words]

  return words

s = "The's quick brown fox jumps over the |||lazy dog."
tokenize(s)

"""## Fetch the corpus
# Lab 1 Assignment:
5. use content
"""

def get_corpus(csv_path):
  df = pd.read_csv(csv_path)
  print("Dataset size: ", len(df))
  print("Dataset columns", df.columns)

  corpus = df.to_dict('records')

  return corpus

"""## Make corpus into Counter"""

from collections import Counter
def get_vocab(corpus):
  vocabulary = Counter()

  for document in corpus:
    tokens = tokenize(document['content'])
    vocabulary.update(tokens)

  return vocabulary

"""## document to vector
Bag-of-words
"""

def doc2vec(doc, vocab):
  words = tokenize(doc)

  vector = []
  for token in dict(vocab).keys():
    if token in words:
      vector.append(1)
    else:
      vector.append(0)
      
  return vector

"""# Lab 1 Assignment: 
4. Replace BoW vectors with TF-IDF vectors
"""

def corpus_doc_count(corpus):
  """
  find the number of documents where the term t appears
  """
  corpus_count = Counter()

  for document in corpus:
    tokens = set(tokenize(document['content']))
    corpus_count.update(tokens)

  corpus_count = dict(corpus_count)

  return corpus_count

def tfidf_doc2vec(doc, vocab, corpus, corpus_count):
  doc_vocab = dict(get_vocab([doc]))

  doc_num_of_words = sum(doc_vocab.values())

  vec = list()
  for word in dict(vocab).keys():
    if word in doc_vocab.keys():
      if_value = doc_vocab[word] / doc_num_of_words
      idf_value = math.log(len(corpus) / corpus_count[word])
      vec.append(if_value * idf_value)

    else:
      vec.append(0)

  return vec

"""## Similarity"""

def cosine_similarity(vec_a, vec_b):
  assert len(vec_a) == len(vec_b)

  if sum(vec_a) == 0 or sum(vec_b) == 0:
    return 0

  a_b = sum(i[0] * i[1] for i in zip(vec_a, vec_b))
  a_2 = sum([i * i for i in vec_a])
  b_2 = sum([i * i for i in vec_b])

  return a_b / (math.sqrt(a_2) * math.sqrt(b_2))

def doc_similarity(doc_a, doc_b, vocab, corpus, corpus_count):
  return cosine_similarity(
      tfidf_doc2vec(doc_a, vocab, corpus=corpus, corpus_count=corpus_count), 
      tfidf_doc2vec(doc_b, vocab, corpus=corpus, corpus_count=corpus_count))

def k_similar(seed_id, vocab, k=5):
  seed_doc = corpus[seed_id]
  print(f"> {seed_doc['title']}")

  corpus_count = corpus_doc_count(corpus)

  similarities = [doc_similarity(
                    seed_doc, doc, vocab, corpus, corpus_count) \
                  for doc in corpus]
  top_indices = sorted(range(len(similarities)), 
                       key=lambda i: similarities[i])[-k:]

  nearest = [[corpus[id], similarities[id]] for id in top_indices]

  for story in reversed(nearest):
    print(f"* {story[0]['title']} ({story[1]})")

corpus = get_corpus("https://raw.githubusercontent.com/bshmueli/108-nlp/master/reuters.csv")
vocab = get_vocab(corpus)

STUDENT_ID = 860908
lab1_index = STUDENT_ID % 1000
print(f'\nStudent ID: 0{STUDENT_ID}')
print(f'computed index: {lab1_index}\n')

k_similar(lab1_index, vocab=vocab, k=5)

