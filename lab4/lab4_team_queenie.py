# -*- coding: utf-8 -*-
"""lab4-Team_Queenie.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SXHJ3oqqcwi_jP61o9DOZERtdu8gWTNa

# Upload credentials.csv
"""

from google.colab import files

uploaded = files.upload()

"""# Calculate average number of tokens"""

import pandas as pd

url = 'https://gist.githubusercontent.com/bshmueli/c99fc0abf56460e644bd610bf3024e83/raw/720285d133c85d94e0aa3fe3edcc199f6d99e3f7/lab4-data.csv'
corpus_df = pd.read_csv(url)

from nltk.tokenize import TweetTokenizer
import numpy as np

tokenizer = TweetTokenizer()

corpus = corpus_df.values[:, 1]
idx = corpus_df.values[:, 0]

num_tokens = list()
for text in corpus:
  tokens = tokenizer.tokenize(text)

  num_tokens.append(len(tokens))

avg_tokens = int(np.array(num_tokens).mean())
print('Average number of tokens: ', avg_tokens)

"""# Authenticate"""

import boto3

CREDENTIAL_FILES = 'credentials.csv'
credentials = pd.read_csv(CREDENTIAL_FILES).to_dict('records')[0]

aws_access_key_id = credentials['Access key ID']
aws_secret_access_key = credentials['Secret access key']

region_name = 'us-east-1' # virginia
endpoint_url = 'https://mturk-requester-sandbox.us-east-1.amazonaws.com'

client = boto3.client(
    'mturk',
    aws_access_key_id=aws_access_key_id,
    aws_secret_access_key=aws_secret_access_key,
    endpoint_url=endpoint_url,
    region_name=region_name
)

"""# Get Balance"""

client.get_account_balance()

"""# Create HIT Type"""

one_minute = 60 # 60 seconds
one_hour = 60 * one_minute
one_day = 24 * one_hour

long_hit_type_response = client.create_hit_type(
        Title='Select a emotion (long text)',
        Description='Choose the emotion that most matches the text.',
        Reward='1.0',
        AutoApprovalDelayInSeconds=60 * one_minute,
        Keywords='Emotion Detection,language,emotion',
        AssignmentDurationInSeconds=30 * one_minute,
        QualificationRequirements=[
            {
                'QualificationTypeId': '00000000000000000071', 
                'Comparator': 'In',
                'LocaleValues': [
                    {
                        'Country': 'TW'
                    },
                    {
                        'Country': 'US'
                    },
                ],
                'RequiredToPreview': True,
                'ActionsGuarded': 'PreviewAndAccept'
            },
        ]
    )

short_hit_type_response = client.create_hit_type(
        Title='Select a emotion (short text)',
        Description='Choose the emotion that most matches the text.',
        Reward='0.5',
        AutoApprovalDelayInSeconds=60 * one_minute,
        Keywords='Emotion Detection,language,emotion',
        AssignmentDurationInSeconds=30 * one_minute,
        QualificationRequirements=[
            {
                'QualificationTypeId': '00000000000000000071', 
                'Comparator': 'In',
                'LocaleValues': [
                    {
                        'Country': 'TW'
                    },
                    {
                        'Country': 'US'
                    },
                ],
                'RequiredToPreview': True,
                'ActionsGuarded': 'PreviewAndAccept'
            },
        ]
    )

long_ = 0
short_ = 0
for i in range(len(corpus)):
    if num_tokens[i] > avg_tokens:
        hit_type_response = long_hit_type_response
        long_ += 1
    else:
        hit_type_response = short_hit_type_response
        short_ += 1
    
    question=f'''<?xml version="1.0" encoding="UTF-8"?>
    <ExternalQuestion xmlns="http://mechanicalturk.amazonaws.com/AWSMechanicalTurkDataSchemas/2006-07-14/ExternalQuestion.xsd">
      <ExternalURL>https://shao-chi.github.io/1082-nctu-nlp/lab4/html/{idx[i]}.html</ExternalURL>
      <FrameHeight>800</FrameHeight>
    </ExternalQuestion>'''

    response = client.create_hit_with_hit_type(
        HITTypeId=hit_type_response['HITTypeId'],
        MaxAssignments=3,
        LifetimeInSeconds=56 * one_day,
        Question=question,
        RequesterAnnotation='VAD',
    )
    print(response)

print('long: ', long_)
print('short: ', short_)

"""# Fetch Results"""

from time import sleep
import xml.etree.ElementTree as ET
import re

hits_paginator = client.get_paginator('list_hits')
assignments_paginator = client.get_paginator('list_assignments_for_hit')

valence = ['Unpleasant', 'Unsatisfied', 'Neutral', 'Pleased', 'Pleasant']
arousal = ['Calm', 'Dull', 'Neutral', 'Wide-awake', 'Excited']
dominance = ['Independent', 'Powerful', 'Neutral', 'Powerlessness', 'Dependent']

result = []

for hits in hits_paginator.paginate():
    for hit in hits['HITs']:
        for assignments in assignments_paginator.paginate(HITId=hit['HITId']):
            for assignment in assignments['Assignments']:
                tmp = dict()

                root = assignment['Answer'].strip('<').strip('>').split('><')
                for i in range(len(root)):
                    if root[i] == 'Answer':
                        question = re.split(r'[><]', root[i+1])[1]
                        answer = re.split(r'[><]', root[i+2])[1]

                        if question == 'CorpusIdx':
                            tmp['id'] = answer

                        if question == 'valence':
                            tmp[question] = valence.index(answer) + 1

                        if question == 'arousal':
                            tmp[question] = arousal.index(answer) + 1

                        if question == 'dominance':
                            tmp[question] = dominance.index(answer) + 1
                
                tmp['time'] = (assignment['SubmitTime'] - assignment['AcceptTime']).seconds

                result.append(tmp)
                # print(result)
            sleep(1)

print(result)

result_df = pd.DataFrame.from_records(result) \
              .groupby(['id']) \
              .agg({'valence': 'mean', 'arousal': 'mean', 'dominance': 'mean', 'time': 'mean','id': 'count'}) 

result_df.columns = ['avg_valence', 'avg_arousal', 'avg_dominance', 'avg_time', 'assignments']
result_df = result_df.reset_index()

result_df

result_df.to_csv('result.csv', index=False)